{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Neural Networks Learn to Do Harmonic Analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is all the code for the circular data project. Included is datagen (for circular and survey), as well as the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import randint\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from scipy.stats import vonmises\n",
    "import numpy.fft as fft \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Class code for the network\n",
    "'''\n",
    "class CircleNet(nn.Sequential): \n",
    "    def __init__(self, res, dropout, n_out, hidden_nodes=4): \n",
    "   '''\n",
    "       res :: int; input layer size\n",
    "       dropout:: ; TODO\n",
    "       n_out::int; output layer size \n",
    "   '''\n",
    "        super(CircleNet, self).__init__()\n",
    "        self.n_in = res\n",
    "        self.affine1 = nn.Linear(self.n_in, hidden_nodes)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.affine3 = nn.Linear(hidden_nodes, n_out)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        '''\n",
    "            Forward feed method\n",
    "            Currently utilises ReLu activation function and softmax for output.\n",
    "        '''\n",
    "        x = F.relu(self.affine1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.affine3(x))\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code blocks are all for data generation. Data generation outputs to files so that we can keep them for consistent testing. Note: data generation for the hypercube distribution is at the end of the file because it is somewhat long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Generate Uniform Circular Data\n",
    "'''\n",
    "num_points = 100 # Total number of data points for each sample\n",
    "num_samples = 10000 # Total number of samples to generate\n",
    "res = 8\n",
    "mean = num_points / res\n",
    "std = math.sqrt(1/res*(1-1/res)) * math.sqrt(num_points)\n",
    "data = np.zeros([num_points,res + 1])\n",
    "output_name = \"uniform8ptnormalised.csv\"\n",
    "\n",
    "# generates uniform \n",
    "# Generates by coosing a random bucket\n",
    "for i in range(num_points): \n",
    "    for j in range(num_samples): \n",
    "        num = randint(0,res-1)\n",
    "        data[i][num]+= 1\n",
    "        data[i][res] = -1\n",
    "    data[i][:-1] = (data[i][:-1]-mean)/std\n",
    "np.savetxt(output_name, data, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Generate unimodal von mesis data.\n",
    "'''\n",
    "\n",
    "num_buckets = 8\n",
    "data = np.zeros([10000,num_buckets+1])\n",
    "std = math.sqrt(1/num_buckets*(1-1/num_buckets))\n",
    "mean = num_points / num_buckets\n",
    "SAMPLE_SIZE = 100\n",
    "kappa = 1.5\n",
    "num_samples = 10000\n",
    "output_name = \"vonmises8ptnormtest.csv\"\n",
    "for i in range(num_samples):\n",
    "    # von mesis always chooses 0 as mean so we apply random rotation to \n",
    "    # get a different mean bucket on each sample set.\n",
    "    rotate_amt = random.random()*2*np.pi\n",
    "    rotate_amt2 = random.random()*2*np.pi # TODO why 2 rotations?\n",
    "    vm_dist = vonmises.rvs(kappa, loc = rotate_amt, size=SAMPLE_SIZE//2)\n",
    "    vm_dist2 = vonmises.rvs(kappa, loc = rotate_amt2, size = SAMPLE_SIZE//2)\n",
    "    for l in range(SAMPLE_SIZE//2):\n",
    "        # determines which buckets to put sample in\n",
    "        vm_dist[l] = ((vm_dist[l] + np.pi)/(2*np.pi))%1*(num_buckets*10)//10\n",
    "        vm_dist2[l] = ((vm_dist2[l] + np.pi)/(2*np.pi))%1*(num_buckets*10)//10\n",
    "        data[i][int(vm_dist[l])] += 1\n",
    "        data[i][int(vm_dist2[l])]+=1\n",
    "    data[i][:-1] = (data[i][:-1] - mean)/std\n",
    "    data[i,-1] = -7\n",
    "np.savetxt(output_name, data, delimiter = \",\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Generate Bimodal Von Mesis distribution.\n",
    "'''\n",
    "\n",
    "# TODO unhardcode number of buckets\n",
    "num_samples = 10000\n",
    "SAMPLE_SIZE = 100\n",
    "output_name = \"bimodal8pt100.csv\"\n",
    "data = np.zeros([num_samples,res+1])\n",
    "for i in range(num_samples):\n",
    "    kappa = 1\n",
    "    # von mesis function always chooses 0 as mean, so we add a random rotation\n",
    "    rotate_amt = random.random()*2*np.pi\n",
    "    vm_dist = vonmises.rvs(kappa, loc = rotate_amt, size=SAMPLE_SIZE)\n",
    "    for l in range(SAMPLE_SIZE):\n",
    "        random_rotate_factor = randint(0,1)\n",
    "        # determines which bucket to place sample in\n",
    "        vm_dist[l] = ((vm_dist[l] + np.pi + np.pi*random_rotate_factor)/(2*np.pi))%1*(res*10)//10\n",
    "    \n",
    "    for k in range(SAMPLE_SIZE):\n",
    "        data[i][int(vm_dist[k])] += 1\n",
    "    data[i][:-1] = (data[i][:-1] - 12)/5\n",
    "    data[i,-1] = -6\n",
    "np.savetxt(output_name, data, delimiter = \",\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Class for loading a previously saved dataset. Loads the data in such a way that \n",
    "    it can be used by the data loader.\n",
    "    data_root::string; the name of the file to load\n",
    "'''\n",
    "class CircularDataset(Dataset): \n",
    "    def __init__(self, data_root): \n",
    "        self.data = []\n",
    "        self.dist_coder = LabelEncoder()\n",
    "        self.dist_list = []\n",
    "        for path in data_root: \n",
    "            array = pd.read_csv(path)\n",
    "            \n",
    "            self.dist_list += [array.values[0,-1]] ##get the distribution type for each file and add it to the list\n",
    "            if len(self.data) == 0: \n",
    "                self.data = array.values\n",
    "            else:\n",
    "                print(self.data.shape)\n",
    "                print(array.values.shape)\n",
    "                self.data = np.concatenate((self.data,array.values), axis = 0)\n",
    "        self.dist_coder.fit(self.dist_list)\n",
    "    def __getitem__(self,idx): \n",
    "        return self.data[idx][0:-1], self.to_one_hot(self.dist_coder,([self.data[idx,-1]]))[0]\n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    def to_one_hot(self, codec, values):\n",
    "        value_idxs = codec.transform(values)\n",
    "        return torch.eye(len(codec.classes_))[value_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next set of code is for the actual training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights of the network with bias.\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.05)\n",
    "        \n",
    "rate = .005\n",
    "batchsize = 100\n",
    "\n",
    "'''\n",
    "    dataset is the list of files to train the network on.\n",
    "    testdata is the list of files to test the network's performance on.\n",
    "    Change these filenames as needed\n",
    "'''\n",
    "dataset = CircularDataset([\"uniform8ptdata.csv\",\"vonmises8ptnorm.csv\"])\n",
    "testdata = CircularDataset([\"uniform8ptnormalisedtest.csv\", \"vonmises8ptnormtest.csv\"])\n",
    "\n",
    "iterator = torch.utils.data.DataLoader(dataset, batch_size = batchsize, shuffle = True)\n",
    "testiterator = torch.utils.data.DataLoader(testdata, batch_size = 1, shuffle = True)\n",
    "# Network arguments: input layer size, dropout, output layer size\n",
    "net = CircleNet(8,0,2)\n",
    "optimizer = optim.Adam(net.parameters(),lr = rate)\n",
    "running_loss = 0\n",
    "netlist = []\n",
    "epochs = 12\n",
    "net.apply(init_weights)\n",
    "net.train()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "for e in range(epochs): \n",
    "    print(running_loss) # TODO what does this do, should we delete it?\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    i = 0\n",
    "    for dist, labels in iterator: \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = net(dist.float())\n",
    "        \n",
    "        loss = loss_func(out,torch.max(labels, 1)[1])\n",
    "       \n",
    "           \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "    for dist, labels in testiterator: \n",
    "        ps = net(dist.float())\n",
    "\n",
    "        i += 1\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "       \n",
    "        top_p2, top_class2 = labels.topk(1, dim = 1)\n",
    "        \n",
    "        equals = top_class == top_class2\n",
    "        \n",
    "        accuracy += torch.sum(equals.float())\n",
    "        \n",
    "    # TODO what precisely does this print?\n",
    "    print(accuracy)\n",
    "    print(i)\n",
    "    accuracy = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Circular Classification Weighting Visualisation\n",
    "    \n",
    "    Once the network finishes training we use this block to output investigate the weights.\n",
    "    TODO throw in the visualisation code here too, idk where it is\n",
    "'''\n",
    "paramlist2 = []\n",
    "for parameter in net.parameters(): \n",
    "    paramlist2 += [parameter]\n",
    "array2 = paramlist2[0].detach().numpy()\n",
    "np.around(np.matmul(array2,fft.ifft(np.identity(8),8)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are some example plots for the output. For explanation please read the report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](document_figures/image4.png)\n",
    "![title](document_figures/image8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Hypercube Classification Weighting Visualisation\n",
    "    \n",
    "    Run this code block once the network has finished training to output the weight matrix,\n",
    "    as well as a plot of the weighting on each hadamard basis.\n",
    "'''\n",
    "paramlist2 = []\n",
    "for parameter in net.parameters(): \n",
    "    paramlist2 += [parameter]\n",
    "array2 = paramlist2[0].detach().numpy()\n",
    "array2\n",
    "basis = np.matmul(array2, inv(hadamard(8)))\n",
    "for a in basis:\n",
    "    plt.plot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some example plots for the output. For explanation please read the report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](document_figures/image7.png)\n",
    "![title](document_figures/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code blocks are for the data generation. Both circular datasets and survey datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "'''\n",
    "    Data generation for the survey data on a hypercube\n",
    "'''\n",
    "\n",
    "# Data Generation\n",
    "# length: int; length of each binary string\n",
    "def generateNormalData(length, sampleSize, sd=1, plot_res = False):\n",
    "    # generate all binary strings of given length\n",
    "    binaryStrings = [''.join(map(str,i)) for i in itertools.product([0,1], repeat=length)]\n",
    "    meanBucket = random.choice(binaryStrings)\n",
    "    \n",
    "    # map each bucket to their distance from the mean bucket\n",
    "    distances = {i : getDistance(meanBucket, i) for i in binaryStrings}\n",
    "    samples = {i: 0 for i in binaryStrings} # sample data\n",
    "    bucket_size = 2 * sd / max(distances.values()) # map buckets from 0 to 2*sd (MAKE THIS A PARAMETER LATER)\n",
    "    \n",
    "    # map each point to their pdf\n",
    "    pdfs = {i : norm.pdf(distances[i] * bucket_size) for i in binaryStrings}\n",
    "    # normalise and map each pdf to some value between 0 and 1\n",
    "    norm_const = sum(pdfs.values()) \n",
    "    normalised_cumu = {i : 0 for i in binaryStrings}\n",
    "    s = 0\n",
    "    for key in pdfs:\n",
    "        normalised_pdf = pdfs[key] / norm_const\n",
    "        normalised_cumu[key] = normalised_pdf + s\n",
    "        s += normalised_pdf\n",
    "    \n",
    "    # Maps the right end point of each bucket to its corresponding binary string\n",
    "    cumu_to_bin = {normalised_cumu[i] : i for i in binaryStrings}\n",
    "    \n",
    "    # Get samples\n",
    "    # find which bin it fits in first\n",
    "    # This works because we iterate from smallest to largest bucket\n",
    "    cdf_arr = sorted(list(cumu_to_bin.keys()))\n",
    "    for n in range(sampleSize):\n",
    "        roll = random.random()\n",
    "        first_bucket = None\n",
    "        for bucket in cdf_arr:\n",
    "            if roll < bucket:\n",
    "                first_bucket = bucket\n",
    "                break\n",
    "        samples[cumu_to_bin[first_bucket]] += 1\n",
    "\n",
    "    # randomly invert all of the keys\n",
    "    # TODO change this so that each digit has an independent chance of inversion\n",
    "    random_inversion = random.random()\n",
    "    if random_inversion < 0.5:\n",
    "        # swap all the ones and zeros\n",
    "        new_samples = {}\n",
    "        for key in samples:\n",
    "            new_key = ''.join(list(map(\n",
    "                lambda x: \"1\" if x == \"0\" else \"0\", key \n",
    "            )))\n",
    "            new_samples[new_key] = samples[key]\n",
    "    \n",
    "    # Optional plotting of distribution\n",
    "    if plot_res:\n",
    "        x = sorted(binaryStrings, key = lambda s: distances[s])\n",
    "        y = [samples[i] for i in x]\n",
    "        plt.plot(x, y)\n",
    "        plt.show()\n",
    "    return samples\n",
    "\n",
    "# Helper function to get the distance between two binary strings\n",
    "# b1, b2::string; Binary strings to find distance between\n",
    "def getDistance(b1, b2):\n",
    "    # Find the differences between each string\n",
    "    if len(b1) != len(b2):\n",
    "        raise Exception(\"b1, b2 not the same length\")\n",
    "    differences = ''\n",
    "    for i in range(len(b1)):\n",
    "        if b1[i] != b2[i]:\n",
    "            differences += '1'\n",
    "        else:\n",
    "            differences += '0'\n",
    "    return sum([int(i) for i in differences])\n",
    "            \n",
    "# Generate Uniform Data\n",
    "'''\n",
    "    Length::int; binary string length. ie dimension of the hypercube\n",
    "    sampleSize::int; number of samples to generate\n",
    "'''\n",
    "def generateUniformData(length, sampleSize):\n",
    "    # Generates a uniform distribution over binary survey results\n",
    "    # Return: dictionary Strings -> ints\n",
    "    binaryStrings = [''.join(map(str,i)) for i in itertools.product([0,1], repeat=length)]\n",
    "    \n",
    "   \n",
    "    surveyChoices = {\n",
    "        i : 0 for i in binaryStrings\n",
    "    }\n",
    "    for n in range(sampleSize):\n",
    "        surveyChoices[random.choice(binaryStrings)] += 1\n",
    "    \n",
    "    return surveyChoices\n",
    "\n",
    "'''\n",
    "    Generates a bunch of normally distributed data and saves it to a file\n",
    "    \n",
    "    length::int; the dimension of the hypercube to generate data on\n",
    "    data_points::int; how many data points to generate\n",
    "    sample_size::int; how many samples each data point should be\n",
    "    name::string; output file name\n",
    "'''\n",
    "def genNormalDataToFile(length, data_points, sample_size, name):\n",
    "    # save to file.\n",
    "    # save in order of binary strings\n",
    "    data = np.zeros([data_points, 2**length+1])\n",
    "    for i in range(data_points):\n",
    "        samples = generateNormalData(length, sampleSize=sample_size)\n",
    "        binaryStrings = sorted(samples.keys())\n",
    "        for j in range(len(binaryStrings)):\n",
    "            bs = binaryStrings[j]\n",
    "            # j is binary string index\n",
    "            data[i][j] = samples[bs]\n",
    "        data[i][-1] = -11 # Arbritrary label\n",
    "    np.savetxt(name, data, delimiter = \",\") \n",
    "\n",
    "'''\n",
    "    Generates a bunch of uniformally distributed data and saves it to a file\n",
    "    \n",
    "    length::int; the dimension of the hypercube to generate data on\n",
    "    data_points::int; how many data points to generate\n",
    "    sample_size::int; how many samples each data point should be\n",
    "    name::string; output file name\n",
    "'''\n",
    "def genUniformDataToFile(length, data_points, sample_size, name):\n",
    "    # save to file.\n",
    "    # save in order of binary strings\n",
    "    data = np.zeros([data_points, 2**length+1])\n",
    "    for i in range(data_points):\n",
    "        samples = generateUniformData(length, sampleSize=sample_size)\n",
    "        binaryStrings = sorted(samples.keys())\n",
    "        for j in range(len(binaryStrings)):\n",
    "            bs = binaryStrings[j]\n",
    "            # j is binary string index\n",
    "            data[i][j] = samples[bs]\n",
    "        data[i][-1] = -10 # Arbritrary label\n",
    "    np.savetxt(name, data, delimiter = \",\") \n",
    "    \n",
    "# Code used to actually generate the data. Uncomment and adjust as needed\n",
    "'''\n",
    "for n in [\"surveyDataNormal3qs.csv\", \"surveyDataNormal3qsTest.csv\"]:\n",
    "    genNormalDataToFile(length=3, data_points=10000, sample_size=250, name=n)\n",
    "\n",
    "for n in [\"surveyDataUniform3qs.csv\", \"surveyDataUniform3qsTest.csv\"]:\n",
    "    genUniformDataToFile(length=3, data_points=10000, sample_size=250, name=n)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](document_figures/image5.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
